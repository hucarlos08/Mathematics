{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hucarlos08/Mathematics/blob/main/LinearRegressionMath.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHNWzA9OI8ob"
      },
      "source": [
        "# Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJqP-wwZI_n2"
      },
      "source": [
        "## Mathematical Development of the Cost Function in Linear Regression:\n",
        "\n",
        "In linear regression, the cost function is typically defined as the sum of the squares of the differences between the actual and predicted values. This is based on the assumption that the relationship between the independent variables (features) and the dependent variable (output) can be approximated linearly.\n",
        "\n",
        "The standard equation for the cost function $E(\\mathbf{w}, w_0)$ in linear regression is:\n",
        "\n",
        "$$\n",
        "E(\\mathbf{w}, w_0) = \\sum_{i=1}^{N} (y_i - (\\mathbf{w}^\\top \\mathbf{x}_i + w_0))^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{w}\\in\\mathbb{R}^D$ is the vector of weights.\n",
        "- $w_0\\in\\mathbb{R}$ is the bias term.\n",
        "- $y_i\\in\\mathbb{R}$ is the actual output value for the i-th sample.\n",
        "- $\\mathbf{x}_i\\in\\mathbb{R}^D$ is the input feature vector for the i-th sample.\n",
        "- $N$ is the total number of samples.\n",
        "\n",
        "### Transforming to Matrix Form:\n",
        "\n",
        "To rewrite this in a more convenient matrix form, we modify the vectors to include the bias term in the calculations:\n",
        "- Redefine $\\mathbf{w}$ as $\\mathbf{w}' = [w_0, \\mathbf{w}^\\top]^\\top$.\n",
        "- Modify each $\\mathbf{x}_i$ to $\\mathbf{x}_i' = [1, \\mathbf{x}_i^\\top]^\\top$.\n",
        "\n",
        "Then, the cost function becomes:\n",
        "\n",
        "$$\n",
        "E(\\mathbf{w}') = \\sum_{i=1}^{N} (y_i - \\mathbf{w}'^\\top \\mathbf{x}_i')^2\n",
        "$$\n",
        "\n",
        "### Expressing in Matrix Form:\n",
        "\n",
        "We introduce the following matrices:\n",
        "- $\\mathbf{X} = [\\mathbf{x}_1', \\mathbf{x}_2', ..., \\mathbf{x}_N']^\\top$ as a $N \\times (D+1)$ matrix, where $D$ is the number of features (excluding the bias term).\n",
        "- $\\mathbf{Y} = [y_1, y_2, ..., y_N]^\\top$ as an $N \\times 1$ vector. In other words\n",
        "$$\n",
        "   \\mathbf{X} =\n",
        "   \\begin{pmatrix}\n",
        "   1 & x_{11} & x_{12} & \\ldots & x_{1d} \\\\\n",
        "   1 & x_{21} & x_{22} & \\ldots & x_{2d} \\\\\n",
        "   \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "   1 & x_{N1} & x_{N2} & \\ldots & x_{Nd}\n",
        "   \\end{pmatrix}\n",
        "$$, and\n",
        "\n",
        "$$\n",
        "   \\mathbf{w} =\n",
        "   \\begin{pmatrix}\n",
        "   w_0 \\\\\n",
        "   w_1 \\\\\n",
        "   \\vdots \\\\\n",
        "   w_d\n",
        "   \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "The cost function in matrix form is:\n",
        "\n",
        "$$\n",
        "E(\\mathbf{w}') = (\\mathbf{Y} - \\mathbf{X}\\mathbf{w}')^\\top (\\mathbf{Y} - \\mathbf{X}\\mathbf{w}')\n",
        "$$\n",
        "\n",
        "This matrix form is beneficial because it simplifies the calculation and implementation of algorithms, particularly with matrix computation tools such as NumPy in Python.\n",
        "\n",
        "### Derivation for Optimization:\n",
        "\n",
        "To find the optimal values of $\\mathbf{w}'$ that minimize $E(\\mathbf{w}')$, we compute the derivative of $E(\\mathbf{w}')$ with respect to $\\mathbf{w}'$ and set it to zero:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial E(\\mathbf{w}')}{\\partial \\mathbf{w}'} = 0\n",
        "$$\n",
        "\n",
        "Solving this derivative leads to the solution for the optimal weights in linear regression, often involving the inversion of the matrix $\\mathbf{X}^\\top \\mathbf{X}$ (or a regularized version to avoid inversion problems).\n",
        "\n",
        "This process forms the basis of what is known as \"ordinary linear regression\" or \"ordinary least squares.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkDdmmhPUgGM"
      },
      "source": [
        "## Useful derivatives rules\n",
        "\n",
        "### 1. Derivative of a Scalar Function with Respect to a Vector\n",
        "\n",
        "When differentiating a scalar function (like our cost function $E(\\mathbf{w}')$) with respect to a vector $\\mathbf{w}$, the result is a vector whose elements are the partial derivatives of the scalar function with respect to each element of the vector.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial \\mathbf{w}} = \\begin{pmatrix} \\frac{\\partial f}{\\partial w_1} \\\\ \\frac{\\partial f}{\\partial w_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial w_n} \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "### 2. Chain Rule\n",
        "\n",
        "The chain rule in multivariable calculus is essential when one variable depends on another variable, which in turn depends on another. In the context of matrices and vectors, if you have a function $f(\\mathbf{w})$ and $\\mathbf{w}$ is a function of another vector $\\mathbf{v}$, then the derivative of $f$ with respect to $\\mathbf{v}$ is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial \\mathbf{v}} = \\frac{\\partial f}{\\partial \\mathbf{w}} \\frac{\\partial \\mathbf{w}}{\\partial \\mathbf{v}}\n",
        "$$\n",
        "\n",
        "### 3. Derivative of Matrix Products\n",
        "\n",
        "If you have a product of matrices and vectors, the derivative of this product with respect to the vector is an essential rule. For example, for $f(\\mathbf{w}) = \\mathbf{A}\\mathbf{w}$, where $\\mathbf{A}$ is a constant matrix and $\\mathbf{w}$ is a vector, the derivative of $f$ with respect to $\\mathbf{w}$ is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial (\\mathbf{A}\\mathbf{w})}{\\partial \\mathbf{w}} = \\mathbf{A}\n",
        "$$\n",
        "\n",
        "### 4. Derivative of a Dot Product\n",
        "\n",
        "For the dot product $\\mathbf{w}^\\top \\mathbf{x}$, the derivative with respect to $\\mathbf{w}$ is simply $\\mathbf{x}$.\n",
        "\n",
        "### 5. Derivative of a Quadratic Function\n",
        "In general if $f(\\mathbf{w})=\\mathbf{w}^\\top A\\mathbf{w}$, then\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial \\mathbf w}=(\\mathbf A + \\mathbf A^\\top)\\mathbf w\n",
        "$$\n",
        "In the case **special** case of a quadratic function, where $\\mathbf{A}$ is symmetric, the derivative with respect to $\\mathbf{w}$ is $2\\mathbf{A}\\mathbf{w}$.\n",
        "\n",
        "### Applying to the Gradient of $E(\\mathbf{w}')$\n",
        "\n",
        "Given our cost function $E(\\mathbf{w}')$, which is quadratic in terms of $\\mathbf{w}'$, we can apply these rules to find its gradient. The process will involve deriving terms that involve the product of matrices and vectors and using the chain rule to handle the quadratic form of the cost function. The result will be a vector indicating the direction in which $E(\\mathbf{w}')$ increases most rapidly, and its magnitude gives an idea of the rate of this increase. In the context of optimization, we aim to move $\\mathbf{w}'$ in the opposite direction to the gradient to minimize $E(\\mathbf{w}')$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3AGaViHBI5X-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOGa3W4PgJOtDEhWloN101q",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
